{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56fede89-fea7-4fa3-af9e-2de0d586543a"}}},{"cell_type":"markdown","source":["# Reading and Writing to Synapse\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n* Describe the connection architecture of Synapse and Spark\n* Configure a connection between Databricks and Synapse\n* Read data from Synapse\n* Write data to Synapse\n\n### Azure Synapse\n- leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data\n- PolyBase T-SQL queries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f6ce8ea-240d-4c7a-8f78-fd163f05e776"}}},{"cell_type":"markdown","source":["## Synapse Connector\n- uses Azure Blob Storage as intermediary\n- uses PolyBase in Synapse\n- enables MPP reads and writes to Synapse from Azure Databricks\n\nNote: The Synapse connector is more suited to ETL than to interactive queries. For interactive and ad-hoc queries, data should be extracted into a Databricks Delta table.\n\n```\n                           ┌─────────┐\n      ┌───────────────────>│ STORAGE │<──────────────────┐\n      │ Storage acc key /  │ ACCOUNT │ Storage acc key / │\n      │ Managed Service ID └─────────┘ OAuth 2.0         │\n      │                         │                        │\n      │                         │ Storage acc key /      │\n      │                         │ OAuth 2.0              │\n      v                         v                 ┌──────v────┐\n┌──────────┐              ┌──────────┐            │┌──────────┴┐\n│ Synapse  │              │  Spark   │            ││ Spark     │\n│ Analytics│<────────────>│  Driver  │<───────────>| Executors │\n└──────────┘  JDBC with   └──────────┘ Configured  └───────────┘\n              username &               in Spark\n              password\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cad821a-5f0c-4ca5-91bd-c7e4603ae119"}}},{"cell_type":"markdown","source":["## SQL DW Connection\n\nThree connections are made to exchange queries and data between Databricks and Synapse\n1. **Spark driver to Synapse**\n   - the Spark driver connects to Synapse via JDBC using a username and password\n2. **Spark driver and executors to Azure Blob Storage**\n   - the Azure Blob Storage container acts as an intermediary to store bulk data when reading from or writing to Synapse\n   - Spark connects to the Blob Storage container using the Azure Blob Storage connector bundled in Databricks Runtime\n   - the URI scheme for specifying this connection must be wasbs\n   - the credential used for setting up this connection must be a storage account access key\n   - the account access key is set in the session configuration associated with the notebook that runs the command\n   - this configuration does not affect other notebooks attached to the same cluster. `spark` is the SparkSession object provided in the notebook\n3. **Synapse to Azure Blob Storage**\n   - Synapse also connects to the Blob Storage container during loading and unloading of temporary data\n   - set `forwardSparkAzureStorageCredentials` to true\n   - the forwarded storage access key is represented by a temporary database scoped credential in the Synapse instance\n   - Synapse connector creates a database scoped credential before asking Synapse to load or unload data\n   - then it deletes the database scoped credential once the loading or unloading operation is done."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f9b66cf-faa7-4ec6-8d86-85e88448398a"}}},{"cell_type":"markdown","source":["## Configuration\n\n### Create Azure Blob Storage\nFollow these steps to [create an Azure Storage Account](https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account?tabs=azure-portal#regenerate-storage-access-keys) and Container. The Synapse connector will use a [Shared Key](https://docs.microsoft.com/en-us/rest/api/storageservices/authorize-with-shared-key) for authorization. Be sure to make note of the **Storage Account Name**, the **Container Name**, and the **Access Key** while working through these steps:\n\n1. Access the Azure Portal\n2. Create a New Resource\n3. Create a Storage account\n4. Make sure to specify the correct *Resource Group* and *Region*. Use any unique string as the  for the **Storage Account Name**\n5. Access Blobs\n6. Create a New Container using any unique string for the **Container Name**\n7. Retrieve the primary **Access Key** for the new Storage Account"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bcfac922-3e3e-4d64-be50-3f08b7874256"}}},{"cell_type":"markdown","source":["In the cell below, enter the **Storage Account Name**, the **Container Name**, and the **Access Key**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ebaf4dd-69f2-4ff3-a801-deaa6465cc98"}}},{"cell_type":"code","source":["storageAccount = dbutils.secrets.get(scope=\"demo\", key=\"storageaccount\")\ncontainerName = \"polybase\"\naccessKey = dbutils.secrets.get(scope=\"demo\", key=\"storagekey\")\n\nspark.conf.set(f\"fs.azure.account.key.{storageAccount}.blob.core.windows.net\", accessKey)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5346275-46cb-477f-ba4d-04e149de709c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Configuration\n\n### Create an Azure Synapse Instance\nFollow these steps to [create an Azure Storage Account](https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account?tabs=azure-portal#regenerate-storage-access-keys) and Container. The Synapse connector will use a [Shared Key](https://docs.microsoft.com/en-us/rest/api/storageservices/authorize-with-shared-key) for authorization. Be sure to make note of the **Storage Account Name**, the **Container Name**, and the **Access Key** while working through these steps:\n\n1. Access the Azure Portal\n2. Create a New Resource\n3. Create a Synapse instance using these attributes:\n   - Use any string for the **Database Name**\n   - Select \"Sample\" as the Source\n   - Select an existing or create a new SQL Server\n5. Access the new Synapse instance\n6. Select Query Editor (preview) and enter the proper credentials\n7. Run these two queries:\n   - Create a Master Key in the SQL DW. This facilitates the SQL DW connection\n\n     `CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'CORRECT-horse-battery-staple';`\n\n   - Use a CTAS to create a staging table for the Customer Table. This query will create an empty table with the same schema as the Customer Table.\n\n     ```\n     CREATE TABLE dbo.DimCustomerStaging\n     WITH\n     ( DISTRIBUTION = ROUND_ROBIN, CLUSTERED COLUMNSTORE INDEX )\n     AS\n     SELECT  *\n     FROM dbo.DimCustomer\n     WHERE 1 = 2\n     ;\n     ```\n7. Access Connection Strings.\n8. Select JDBC and copy the **JDBC URI**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de1ed3d6-3f42-4f6b-bc54-96d5a309e195"}}},{"cell_type":"markdown","source":["In the cell below, we'll access the JDBC URI stored securely in Key Vault.\nNote that the table we will be using has already been defined."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10f664c0-5baa-45fd-a096-d580fad8f9bc"}}},{"cell_type":"code","source":["tableName = \"dbo.DimCustomer\"\njdbcURI = dbutils.secrets.get(scope=\"demo\", key=\"jdbc\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77c50f91-4725-4852-abe0-b3ff93fc68a7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Read from the Customer Table\n\nNext, use the Synapse Connector to read data from the Customer Table.\n\nUse the read to define a tempory table that can be queried.\n\nNote:\n\n- the connector uses a caching directory on the Azure Blob Container.\n- `forwardSparkAzureStorageCredentials` is set to `true` so that the Synapse instance can access the blob for its MPP read via Polybase"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe2051d7-f6cd-40e5-87fc-d9b20439ef71"}}},{"cell_type":"code","source":["cacheDir = f\"wasbs://{containerName}@{storageAccount}.blob.core.windows.net/cacheDir\"\n\ncustomerDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName)\n  .load())\n\ncustomerDF.createOrReplaceTempView(\"customer_data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"668dd1db-e3d5-4bf4-ad68-45baaf0c6240"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Use SQL queries to count the number of rows in the Customer table and to display table metadata."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bd145ad-ab58-4cda-a917-9123339e6a19"}}},{"cell_type":"code","source":["%sql\nselect count(*) from customer_data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98cb4f85-fa2a-470e-b339-4767267b575d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\ndescribe customer_data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"132f8928-6610-4000-ae2a-5027e61dcfc1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that `CustomerKey` and `CustomerAlternateKey` use a very similar naming convention."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc255c6c-b087-49b5-b7b3-2ee18680aa8a"}}},{"cell_type":"code","source":["%sql\nselect CustomerKey, CustomerAlternateKey from customer_data limit 10;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c07bcbc9-510e-4ae8-8f38-9531c2fedf0f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In a situation in which we may be merging many new customers into this table, we can imagine that we may have issues with uniqueness with regard to the `CustomerKey`. Let us redefine `CustomerAlternateKey` for stronger uniqueness using a [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).\n\nTo do this we will define a UDF and use it to transform the `CustomerAlternateKey` column. Once this is done, we will write the updated Customer Table to a Staging table.\n\n**Note:** It is a best practice to update the Synapse instance via a staging table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab42ad98-ac4a-419a-87d1-ff9e8a3dab28"}}},{"cell_type":"code","source":["import uuid\n\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import udf\n\nuuidUdf = udf(lambda : str(uuid.uuid4()), StringType())\ncustomerUpdatedDF = customerDF.withColumn(\"CustomerAlternateKey\", uuidUdf())\ndisplay(customerUpdatedDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7924d59e-2016-47fa-a6f2-c7a11b6114ca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Use the Polybase Connector to Write to the Staging Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62b07116-a4e0-485f-b95f-599739dc702b"}}},{"cell_type":"code","source":["(customerUpdatedDF.write\n  .format(\"com.databricks.spark.sqldw\")\n  .mode(\"overwrite\")\n  .option(\"url\", jdbcURI)\n  .option(\"forward_spark_azure_storage_credentials\", \"true\")\n  .option(\"dbtable\", tableName + \"Staging\")\n  .option(\"tempdir\", cacheDir)\n  .save())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2deef68-686a-4b71-af37-c5a7087846a1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Read and Display Changes from Staging Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"304f84cb-2fe8-46ce-8b9b-238623b21336"}}},{"cell_type":"code","source":["customerTempDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName + \"Staging\")\n  .load())\n\ncustomerTempDF.createOrReplaceTempView(\"customer_temp_data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1838f76e-21be-4666-bf9c-6099dd6d0559"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nselect CustomerKey, CustomerAlternateKey from customer_temp_data limit 10;\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56dc439e-9b3d-44fe-be1f-b0cc0a3cf034"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b30d5b7-9fae-4a7a-9f4d-bcf407dd7c8d"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Synapse","dashboards":[],"language":"python","widgets":{},"notebookOrigID":3243488855258190}},"nbformat":4,"nbformat_minor":0}
