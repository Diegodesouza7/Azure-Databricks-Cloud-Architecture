{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99c64ec0-3a2d-4f26-88ce-bbabc1a74395"}}},{"cell_type":"markdown","source":["# Structured Streaming with Azure EventHubs \n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n* Establish a connection with Event Hubs in Spark\n* Subscribe to and configure an Event Hubs stream\n* Parse JSON records from Event Hubs\n\n## Library Requirements\n\nThe Maven library with coordinate `com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.17`\n\n## Resources\n- [Docs for Azure Event Hubs connector](https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/streaming-event-hubs)\n- [Documentation on how to install Maven libraries](https://docs.azuredatabricks.net/user-guide/libraries.html#maven-or-spark-package)\n- [Spark-EventHub debugging FAQ](https://github.com/Azure/azure-event-hubs-spark/blob/master/FAQ.md)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7943ef70-6670-407b-8334-450a9974dd9f"}}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our classroom and set up a local streaming file read that we'll be writing to Event Hubs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc5b709b-39c7-489a-8f66-45594392443a"}}},{"cell_type":"code","source":["%run ./Includes/Streaming-Demo-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53bd7044-07e3-4a2d-97b7-0a2222bcde40"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Azure Event Hubs</h2>\n\nMicrosoft Azure Event Hubs is a fully managed, real-time data ingestion service.\nYou can stream millions of events per second from any source to build dynamic data pipelines and immediately respond to business challenges.\nIt integrates seamlessly with a host of other Azure services.\n\nEvent Hubs can be used in a variety of applications such as\n* Anomaly detection (fraud/outliers)\n* Application logging\n* Analytics pipelines, such as clickstreams\n* Archiving data\n* Transaction processing\n* User telemetry processing\n* Device telemetry streaming\n* <b>Live dashboarding</b>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7cdef00-55fd-4307-ba92-dd50ddcc7e97"}}},{"cell_type":"markdown","source":["### Define Connection Strings and Create Configuration Object\n\nThis cell uses a connection string to create a simple `EventHubsConf` object, which will be used to connect.\n\nNote that the code below uses DB Utils secrets to load in the Event Hub connection string previously loaded into Azure Key Vault.\n\nTo run this notebook, you'll need to configure Event Hubs and provide the relavent information in the following format:\n```\nEndpoint=sb://<event_hubs_namespace>.servicebus.windows.net/;SharedAccessKeyName=<key_name>;SharedAccessKey=<signing_key>=;EntityPath=<event_hubs_instance>\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76053f3a-a365-4d3a-991e-a44ef7600630"}}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.eventhubs.{EventHubsConf, EventPosition}\n\nval connectionString = dbutils.secrets.get(scope=\"demo\", key=\"ehConnectionString\")\n\nval ehWriteConf = EventHubsConf(connectionString)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b80d0bf-ba75-449e-9e2d-73c3cf6da196"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Write Stream to Event Hub to Produce Stream\n\nBelow, we configure a streaming write to Event Hubs. Refer to the docs for additional ways to [write data to Event Hubs](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/structured-streaming-eventhubs-integration.md#writing-data-to-eventhubs)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca604c62-6261-4c1a-915e-d81bf87ff263"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.streaming.Trigger.ProcessingTime\n\nval checkpointPath = userhome + \"/event-hub/write-checkpoint\"\ndbutils.fs.rm(checkpointPath,true)\n\nactivityStreamDF\n  .writeStream\n  .format(\"eventhubs\")\n  .outputMode(\"update\")\n  .options(ehWriteConf.toMap)\n  .trigger(ProcessingTime(\"25 seconds\"))\n  .option(\"checkpointLocation\", checkpointPath)\n  .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdd49b4b-69dc-455c-8c39-d653f6b01986"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Event Hubs Configuration\n\nAbove, a simple `EventHubsConf` object is used to write data. There are [numerous additional options for configuration](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/structured-streaming-eventhubs-integration.md#eventhubsconf). Below, we specify an `EventPosition` ([docs](https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/streaming-event-hubs#eventposition)) and limit our throughput by setting `MaxEventsPerTrigger`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1532f81b-7355-4c6d-9318-370b98598b9c"}}},{"cell_type":"code","source":["%scala\n\nval eventHubsConf = EventHubsConf(connectionString)\n  .setStartingPosition(EventPosition.fromStartOfStream)\n  .setMaxEventsPerTrigger(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1ba1ac8-397e-446c-8234-cc742239418c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### READ Stream using EventHub\n\nThe `readStream` method is a <b>transformation</b> that outputs a DataFrame with specific schema specified by `.schema()`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c9f1a60-38c7-4405-af8d-73a18cbf1882"}}},{"cell_type":"code","source":["%scala\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\nval eventStreamDF = spark.readStream\n  .format(\"eventhubs\")\n  .options(eventHubsConf.toMap)\n  .load()\n\neventStreamDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc55e8ef-92b9-47b6-9051-a90cbc3432e0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Most of the fields in this response are metadata describing the state of the Event Hubs stream. We are specifically interested in the `body` field, which contains our JSON payload.\n\nNoting that it's encoded as binary, as we select it, we'll cast it to a string."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80af2e8d-712a-4ddb-ac32-3fd14d792ce5"}}},{"cell_type":"code","source":["%scala\nval bodyDF = eventStreamDF.select('body.cast(\"STRING\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7cd2237-e294-4df1-ac2c-52e314d4fbeb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Each line of the streaming data becomes a row in the DataFrame once an <b>action</b> such as `writeStream` is invoked.\n\nNotice that nothing happens until you engage an action, i.e. a `display()` or `writeStream`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1a6cb5a-76a7-428b-af54-651f8324a963"}}},{"cell_type":"code","source":["%scala\ndisplay(bodyDF, streamName= \"bodyDF\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d08d1f5-1314-4fc1-999c-f1ca83a50895"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["While we can see our JSON data now that it's cast to string type, we can't directly manipulate it.\n\nBefore proceeding, stop this stream. We'll continue building up transformations against this streaming DataFrame, and a new action will trigger an additional stream."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b2ca6c8-c0e6-47aa-a6c1-cd9f7479e235"}}},{"cell_type":"code","source":["%scala\nfor (s <- spark.streams.active if s.name == \"bodyDF\") s.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdca1cf0-d773-4354-a9f2-c9d18ac27c67"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n## <img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Parse the JSON payload\n\nThe EventHub acts as a sort of \"firehose\" (or asynchronous buffer) and displays raw data in the JSON format.\n\nIf desired, we could save this as raw bytes or strings and parse these records further downstream in our processing.\n\nHere, we'll directly parse our data so we can interact with the fields.\n\nThe first step is to define the schema for the JSON payload.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Both time fields are encoded as `LongType` here because of non-standard formatting."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f405b3a3-5090-42d6-b4b5-dbf092200b7b"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType, DoubleType}\n\nlazy val schema = StructType(List(\n  StructField(\"Arrival_Time\", LongType),\n  StructField(\"Creation_Time\", LongType),\n  StructField(\"Device\", StringType),\n  StructField(\"Index\", LongType),\n  StructField(\"Model\", StringType),\n  StructField(\"User\", StringType),\n  StructField(\"gt\", StringType),\n  StructField(\"x\", DoubleType),\n  StructField(\"y\", DoubleType),\n  StructField(\"z\", DoubleType),\n  StructField(\"geolocation\", StructType(List(\n    StructField(\"PostalCode\", StringType),\n    StructField(\"StateProvince\", StringType),\n    StructField(\"city\", StringType),\n    StructField(\"country\", StringType)))),\n  StructField(\"id\", StringType)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40dceddc-52f6-439b-bc33-57cebd9cb3e5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Parse the data\n\nNext we can use the function `from_json` to parse out the full message with the schema specified above.\n\nWhen parsing a value from JSON, we end up with a single column containing a complex object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d5f1372-9b5f-40ff-b84a-be49635281d5"}}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.sql.functions.from_json\n\nval parsedEventsDF = bodyDF.select(\n  from_json('body, schema).alias(\"json\"))\n\nparsedEventsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acab5d32-54fe-4f8e-9e66-954e22dcd485"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that we can further parse this to flatten the schema entirely and properly cast our time fields."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74f20f48-8721-4ba4-aa9b-b41c187a72cf"}}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.sql.functions.{from_unixtime, col}\n\nval flatSchemaDF = parsedEventsDF\n  .select(from_unixtime(col(\"json.Arrival_Time\")/1000).alias(\"Arrival_Time\").cast(\"timestamp\"),\n          (col(\"json.Creation_Time\")/1E9).alias(\"Creation_Time\").cast(\"timestamp\"),\n          col(\"json.Device\").alias(\"Device\"),\n          col(\"json.Index\").alias(\"Index\"),\n          col(\"json.Model\").alias(\"Model\"),\n          col(\"json.User\").alias(\"User\"),\n          col(\"json.gt\").alias(\"gt\"),\n          col(\"json.x\").alias(\"x\"),\n          col(\"json.y\").alias(\"y\"),\n          col(\"json.z\").alias(\"z\"),\n          col(\"json.id\").alias(\"id\"),\n          col(\"json.geolocation.country\").alias(\"country\"),\n          col(\"json.geolocation.city\").alias(\"city\"),\n          col(\"json.geolocation.PostalCode\").alias(\"PostalCode\"),\n          col(\"json.geolocation.StateProvince\").alias(\"StateProvince\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fe80294-cb95-423a-a89c-75b0da178c8b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This flat schema provides us the ability to view each nested field as a column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82654722-b93d-468f-a343-30334b3dcf01"}}},{"cell_type":"code","source":["%scala\ndisplay(flatSchemaDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebdb6b40-e899-4c41-9d36-0e2b8cb5b316"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Stop all active streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"347e4e4f-d9c0-45d0-90be-5e4b33d92e5c"}}},{"cell_type":"code","source":["%scala\nfor (s <- spark.streams.active)\n  s.stop\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf1c134b-7cff-4749-94da-914bffbb7eb1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95e26477-e8c2-4883-8d02-42ae554f5cae"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Streaming-With-Event-Hubs-Demo","dashboards":[],"language":"python","widgets":{},"notebookOrigID":3243488855258159}},"nbformat":4,"nbformat_minor":0}
